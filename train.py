import numpy as npimport tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import modelsimport Connect4, Memoryfrom models import DQN"""train.pyTraining loop for our DQN network. Code is borrowed heavily fromhttps://towardsdatascience.com/playing-connect-4-with-deep-q-learning-76271ed663ca,with modifications to fit our game representation and network structure."""# Pulled from TDS articledef compute_loss(logits, actions, rewards):     neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions)    loss = tf.reduce_mean(neg_logprob * rewards)    return loss@tf.functiondef train_step(model, optimizer, observations, actions, rewards):    with tf.GradientTape() as tape:      # Forward propagate through the agent network                logits = model(observations)        loss = compute_loss(logits, actions, rewards)        grads = tape.gradient(loss, model.trainable_variables)                optimizer.apply_gradients(zip(grads, model.trainable_variables))        def get_action(model, observation, epsilon):    #determine whether model action or random action based on epsilon    act = np.random.choice(['model','random'], 1, p=[1-epsilon, epsilon])[0]    observation = np.array(observation).reshape(1,6,7,1)    logits = model.predict(observation)    prob_weights = tf.nn.softmax(logits).numpy()        if act == 'model':        action = list(prob_weights[0]).index(max(prob_weights[0]))    if act == 'random':        action = np.random.choice(7)            return action, prob_weights[0]def random_turn(connect4):    col = np.random.choice([0, 1, 2, 3, 4, 5, 6])    connect4.drop_piece(col)    ### Training of modelenv = Connect4()### Instatiate modelsmodel = DQN()memory = Memory()num_episodes = 1000epsilon = 1reward = 0win_count = 0optimizer = keras.optimizers.Adam(0.001)for episode in range(num_episodes):        # Start new game    env.new_board()    observation = env.board    memory.clear()        epsilon = epsilon * .9998        # Play game    while True:        action, _ = get_action(model, observation, epsilon)        env.drop_piece(action)                random_turn(env)        observation = env.board                done = env.check_win()                # Set rewards for wins and losses        if not done[0]:            reward = 0        elif 'y' in done[1]: # random player wins            reward = -20        elif 'r' in done[1]: # network wins            win_count += 1            reward = 20                    # TODO: deal with ties ?        memory.add_to_memory(observation, action, reward)                if done[0]:                        # train network            train_step(model, optimizer,                       np.array(memory.observations),                       np.array(memory.actions),                       memory.rewards)            break        ## TODO: save modelmodel.save('models/DQN.keras')